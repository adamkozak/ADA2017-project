{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon scraper\n",
    "#### Description:\n",
    "As we do not have informations about book publishing date, category to which is assigned, keywords that could describe a book we decided to scrape this information. We created an account on Amazon Associate - Product Advertising API just to be able to access the Amazon database even if it is regulated by number of request or a rate of requests. Product Advertising API does not provide explicitly categories and keywords (50% of responses for requests made contain html of the product website - we can get categories and keywords using that), so we have to scrape Amazon product website to obtain the remaining ~50% of information. \n",
    "\n",
    "Scraping Amazon website it is really hard since they got an anti-scraping system which rejects our request after some number of request. That makes this process not-easy-to-do task, as it is hard to make it automatic. So it is more or less iterative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_scraper import AmazonScraper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read API Credentials\n",
    "with open('/home/adam/EPFL_courses/PAAPICredentials.csv', newline='') as csvfile:\n",
    "    file = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "    for ind, row in enumerate(file):\n",
    "        if ind == 1:\n",
    "            Access_Key, Secret_Key, user_id = row[0].split(',')\n",
    "\n",
    "# Initialize AmazonScraper instance\n",
    "amzn = AmazonScraper(Access_Key, Secret_Key, user_id, Region='US', MaxQPS=0.9)#, Timeout=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read file with books id and reviews number\n",
    "books_df = pd.read_csv(\"number_of_reviews_per_book.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test API\n",
    "p = amzn.lookup(ItemId='0984588205')\n",
    "p.product.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data collector that uses API\n",
    "http = urllib3.PoolManager(headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'})\n",
    "# Set start and end index of book id in loaded previously file\n",
    "start_iter, end_iter = 20000, 22000\n",
    "iter_num = len(books_df['Book_id'].values[start_iter:end_iter])\n",
    "\n",
    "for ind, asin in enumerate(books_df['Book_id'].values[start_iter:end_iter]):\n",
    "    print(asin)\n",
    "    # Try to extract as much as we can usign API\n",
    "    try:\n",
    "        p = amzn.lookup(ItemId=asin)\n",
    "        try:\n",
    "            author = p.product.author\n",
    "        except:\n",
    "            author = \"\"\n",
    "        try:\n",
    "            title = p.product.title\n",
    "        except:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            brand = p.product.brand\n",
    "        except:\n",
    "            brand = \"\"\n",
    "        try:\n",
    "            release_date = p.product.release_date.isoformat()\n",
    "        except:\n",
    "            try:\n",
    "                release_date = p.product.publication_date.isoformat()\n",
    "            except:\n",
    "                release_date = \"\"\n",
    "        try:\n",
    "            publisher = p.product.publisher\n",
    "        except:\n",
    "            publisher = \"\"\n",
    "        try:\n",
    "            categories = [cat_list for cat_list in [[cat.string for cat in cat_list.find_all(\"a\")] \n",
    "                                                  for cat_list in p.soup.find_all(\"span\", \"zg_hrsr_ladder\")]]\n",
    "        except:\n",
    "            r = http.request('GET', 'http://www.amazon.com/dp/' + asin)\n",
    "            bs = BeautifulSoup(r.data, \"lxml\")\n",
    "            try:\n",
    "                categories = [cat_list for cat_list in [[cat.string for cat in cat_list.find_all(\"a\")] \n",
    "                                                      for cat_list in bs.find_all(\"span\", \"zg_hrsr_ladder\")]]\n",
    "\n",
    "            except:\n",
    "                categories = [] \n",
    "                            \n",
    "        try:\n",
    "            keywords = list(set(map(str.strip, map(str.lower, re.split('; |,|\\*|/|;|-', p.soup.find(attrs={\"name\":\"keywords\"})['content'].replace(\".\", \"\"))))))\n",
    "        except:\n",
    "            try:\n",
    "                keywords = list(set(map(str.strip, map(str.lower, re.split('; |,|\\*|/|;|-', \n",
    "                                                    bs.find(attrs={\"name\":\"keywords\"})['content'].replace(\".\", \"\"))))))\n",
    "            except:\n",
    "                keywords = []    \n",
    "\n",
    "\n",
    "        tmp = {'asin': asin,\n",
    "               'author': author,\n",
    "               'title': title,\n",
    "               'brand': brand,\n",
    "               'release_date': release_date,\n",
    "               'publisher': publisher,\n",
    "               'categories': categories,\n",
    "               'keywords': keywords,\n",
    "              }\n",
    "\n",
    "        with open('product_categories.json', 'a') as fp:\n",
    "            json.dump(tmp, fp)\n",
    "            fp.write(\"\\n\")\n",
    "\n",
    "    except:\n",
    "        print(\"ASIN NOT FOUND\")\n",
    "    print(\"Processed {0}/{1} books\".format(ind+1, iter_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Amazon website scraper\n",
    "http = urllib3.PoolManager(headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'})\n",
    "timeout = 3\n",
    "# It uses an helper file with records of products, rewrite those that are complete and try to scrape information for those that do not contain them \n",
    "with open('product_categories_2.json', 'r') as file:\n",
    "    for line in file:\n",
    "        tmp = eval(line)\n",
    "        if tmp['categories'] == [] or tmp['keywords'] == []:\n",
    "            print(tmp['asin'])\n",
    "            flag = 1\n",
    "            r = http.request('GET', 'http://www.amazon.com/dp/' + tmp['asin'])\n",
    "            bs = BeautifulSoup(r.data, \"lxml\")\n",
    "            while flag == 1:\n",
    "                try:\n",
    "                    tmp['categories'] = [cat_list for cat_list in [[cat.string for cat in cat_list.find_all(\"a\")] \n",
    "                                                      for cat_list in bs.find_all(\"span\", \"zg_hrsr_ladder\")]]\n",
    "\n",
    "                    tmp['keywords'] = list(set(map(str.strip, map(str.lower, re.split('; |,|\\*|/|;|-', \n",
    "                                                    bs.find(attrs={\"name\":\"keywords\"})['content'].replace(\".\", \"\"))))))\n",
    "                    flag = 0\n",
    "                    timeout = 3\n",
    "                except:\n",
    "                    timeout = timeout + 3\n",
    "                    print(\"Error, can get connection, increasing timeout = {}\".format(timeout))\n",
    "                    time.sleep(timeout + np.random.random())\n",
    "                    http = urllib3.PoolManager(headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0',\n",
    "                                                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                                                        'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br'})\n",
    "                    flag = 1\n",
    "            \n",
    "            time.sleep(1 + np.random.random())\n",
    "        with open('product_categories_enhanced.json', 'a') as fp:\n",
    "            json.dump(tmp, fp)\n",
    "            fp.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
