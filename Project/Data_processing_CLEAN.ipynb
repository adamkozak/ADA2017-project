{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.4.5 :: Anaconda custom (64-bit)\n"
     ]
    }
   ],
   "source": [
    "# To use Spark 1.6.3 in Jupyter Notebook we have to use Python 3.4\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# USER to determine spark and dataset directories\n",
    "import getpass\n",
    "USER = getpass.getuser()\n",
    "if USER == 'Tarmo':\n",
    "    SPARK_DIR = 'C:/Users/Tarmo/Documents/Lausanne/CS-401_applied_data_analysis/spark/spark-1.6.3-bin-hadoop2.6'\n",
    "else:\n",
    "    SPARK_DIR = '/home/adam/EPFL_courses/spark-1.6.3-bin-hadoop2.6'\n",
    "# Add your dirs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Tarmo/Documents/Lausanne/CS-401_applied_data_analysis/spark/spark-1.6.3-bin-hadoop2.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPARK_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "#### Spark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(SPARK_DIR)\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime  #to_timestamp, pyspark 2.2 functiona "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import json \n",
    "import gzip\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Spark and SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of dataset and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"/media/adam/B236CB1D36CAE209/Studia/ADA/reviews_Books_5.json\"\n",
    "METADATA_DIR = \"/media/adam/B236CB1D36CAE209/Studia/ADA/meta_Books.json\"\n",
    "\n",
    "# Load the dataset and create RDDs\n",
    "text_file = sc.textFile(DATASET_DIR)\n",
    "# Convert previously read text file to json DataFrame\n",
    "df = sqlContext.read.json(text_file)\n",
    "\n",
    "# Load metadata for dataset and convert it to DataFrame\n",
    "metadata = sc.textFile(METADATA_DIR)\n",
    "metadata_df = sqlContext.read.json(metadata)\n",
    "\n",
    "# Register DataFrames as tables to use those names in SQL-type queries\n",
    "sqlContext.registerDataFrameAsTable(metadata_df, \"metadata\")\n",
    "sqlContext.registerDataFrameAsTable(df, \"dataset\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.alias('a').join(metadata_df.alias('b'),col('b.asin') == col('a.asin')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of RDDs\n",
    "text_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Schema of the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bunch of castings, reviewTime: string->date, unixReviewTime: int->timestamp\n",
    "# We do this to be able to better filter and manipulate the data\n",
    "df = df.withColumn('unixReviewTime', from_unixtime(df['unixReviewTime']))\n",
    "df = df.withColumn('reviewTime', to_date(df['unixReviewTime']))\n",
    "df = df.withColumn('unixReviewTime', df['unixReviewTime'].cast('timestamp'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at couple of records, just to be sure that we obtained what we wanted\n",
    "df.select(\"reviewTime\", 'reviewText', 'unixReviewTime').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of review per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing an average length of review per day\n",
    "aTuple = (0, 0)\n",
    "avg = df.select(\"reviewTime\", 'reviewText').rdd.map(lambda row: (row.reviewTime, len(row.reviewText)))\n",
    "avg = avg.aggregateByKey(aTuple, lambda a,b: (a[0] + b, a[1] + 1), lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "avg = avg.mapValues(lambda v: v[0]/v[1])\n",
    "avg = avg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of record\n",
    "len(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing acquired data using Pandas\n",
    "avg_len = pd.DataFrame(avg, columns=['Date', 'Avg_length'])\n",
    "avg_len['Date'] = pd.to_datetime(avg_len['Date'])\n",
    "avg_len.set_index('Date', inplace=True)\n",
    "avg_len.sort_index(inplace=True)\n",
    "avg_len.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to file not to compute this one more time\n",
    "avg_len.to_csv(\"avg_length_review_by_day.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of reviews per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_data = avg_len.groupby(avg_len.index.to_period('M')).mean()\n",
    "monthly_data.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of reviews between 2012 and 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_data['2012':'2013'].plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "monthly_data.to_csv(\"avg_length_review_by_month.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (Successful) attempt to filter by timestamp \n",
    "tmp = df.rdd.filter(lambda row: row.unixReviewTime > pd.to_datetime('2012-05')\n",
    "                     and row.unixReviewTime < pd.to_datetime('2013'))\n",
    "tmp.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of reviews per each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of reviews per each day\n",
    "number_of_reviews = df.rdd.map(lambda row: (row.reviewTime, 1)).reduceByKey(lambda a, b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing the data\n",
    "rev_num = pd.DataFrame(number_of_reviews, columns=['Date', 'Number of reviews'])\n",
    "rev_num['Date'] = pd.to_datetime(rev_num['Date'])\n",
    "rev_num.set_index('Date', inplace=True)\n",
    "rev_num.sort_index(inplace=True)\n",
    "rev_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data not to compute over and over\n",
    "rev_num.to_csv(\"number_of_reviews_per_day.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of reviews per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monthly_data_reviews = rev_num.groupby(rev_num.index.to_period('M')).sum()\n",
    "monthly_data_reviews.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "monthly_data_reviews.to_csv(\"number_of_reviews_per_month.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of reviews per book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_reviews_per_book = df.rdd.map(lambda row: (row.asin, 1)).reduceByKey(lambda a, b: a+b).collect()\n",
    "len(number_of_reviews_per_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing the data using Pandas and saving it to csv file\n",
    "df_number_of_reviews_per_book = pd.DataFrame(number_of_reviews_per_book, columns=['Book_id', 'Number of reviews'])\n",
    "df_number_of_reviews_per_book.sort_values('Number of reviews', ascending=False, inplace = True)\n",
    "df_number_of_reviews_per_book.to_csv(\"number_of_reviews_per_book.csv\", index=False)\n",
    "df_number_of_reviews_per_book.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram of numbers of reviews\n",
    "plt.hist(np.array(df_number_of_reviews_per_book['Number of reviews'].values), bins = 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic statistics for this variable\n",
    "stats.describe(df_number_of_reviews_per_book['Number of reviews'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "sns.boxplot(df_number_of_reviews_per_book['Number of reviews'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 30 books ranking based of number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_books = df.rdd.map(lambda row: (row.asin, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda wc: -wc[1]).take(30)\n",
    "\n",
    "top_books_df = sqlContext.createDataFrame(top_books, ['asin', 'rew_num'])\n",
    "sqlContext.registerDataFrameAsTable(top_books_df, \"top_books\")\n",
    "top_books_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='1940026016', title='The Atlantis Gene: A Thriller (The Origin Mystery, Book 1)'),\n",
       " Row(asin='0439023483', title='The Hunger Games (The Hunger Games, Book 1)'),\n",
       " Row(asin='0316206849', title=\"The Cuckoo's Calling\"),\n",
       " Row(asin='1469984202', title='Wool - Omnibus Edition'),\n",
       " Row(asin='0141039280', title='The Help'),\n",
       " Row(asin='0385537859', title='Inferno'),\n",
       " Row(asin='038536315X', title='Sycamore Row'),\n",
       " Row(asin='0312853238', title=\"Ender's Game (The Ender Quintet)\"),\n",
       " Row(asin='0007444117', title='Allegiant (Divergent, #3)'),\n",
       " Row(asin='0316055433', title='The Goldfinch: A Novel (Pulitzer Prize for Fiction)'),\n",
       " Row(asin='0007386648', title='Unbroken'),\n",
       " Row(asin='030758836X', title='Gone Girl'),\n",
       " Row(asin='0345803485', title='Fifty Shades of Grey: Book One of the Fifty Shades Trilogy'),\n",
       " Row(asin='0425263924', title='Entwined with You (Crossfire, Book 3)'),\n",
       " Row(asin='0857521012', title='The Light Between Oceans'),\n",
       " Row(asin='0061950726', title='Orphan Train: A Novel'),\n",
       " Row(asin='0849922070', title=\"Heaven is for Real Movie Edition: A Little Boy's Astounding Story of His Trip to Heaven and Back\"),\n",
       " Row(asin='0345803493', title='Fifty Shades Darker'),\n",
       " Row(asin='147674355X', title='Hopeless'),\n",
       " Row(asin='0143170090', title='The Girl with the Dragon Tattoo'),\n",
       " Row(asin='0307943232', title='The Racketeer'),\n",
       " Row(asin='0375831002', title='The Book Thief'),\n",
       " Row(asin='0439023513', title='Mockingjay (The Final Book of The Hunger Games)'),\n",
       " Row(asin='1442362383', title='Doctor Sleep: A Novel'),\n",
       " Row(asin='1442359315', title=\"Proof of Heaven: A Neurosurgeon's Near-Death Experience and Journey into the Afterlife\"),\n",
       " Row(asin='0002007770', title='Water For Elephants'),\n",
       " Row(asin='0007442920', title='Insurgent (Divergent)'),\n",
       " Row(asin='0345803507', title='Fifty Shades Freed: Book Three of the Fifty Shades Trilogy'),\n",
       " Row(asin='144235948X', title='Beautiful Disaster'),\n",
       " Row(asin='0399159347', title=\"The Husband's Secret\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the dataset table with the metadata table to see titles of the most popular books\n",
    "sqlContext.sql(\"select t.asin, m.title from metadata m join top_books t on m.asin=t.asin\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select t.asin, m.title from metadata m join top_books t limit 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two methods of filtering by ids of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_reviewed_books_id_top = df_number_of_reviews_per_book[:30]\n",
    "most_reviewed_top = df.rdd.filter(lambda row: row.asin in list(most_reviewed_books_id_top.Book_id))\n",
    "            .map(lambda row: (row.asin, row.reviewTime)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_reviewed_top_2 = sqlContext.sql(\"select asin, reviewTime from dataset where asin in \" + \n",
    "               str(tuple(most_reviewed_books_id_top.Book_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data for top30 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing the data using Pandas\n",
    "most_reviewed_books_top30_df = pd.DataFrame(most_reviewed_top, columns=['asin', 'reviewTime'])\n",
    "# Convert to datetime type\n",
    "most_reviewed_books_top30_df['reviewTime'] = pd.to_datetime(most_reviewed_books_top30_df['reviewTime'])\n",
    "# Assign number of review to compute the sum\n",
    "most_reviewed_books_top30_df['Number_of_reviews'] = 1\n",
    "# Create monthly period for aggregation purpose\n",
    "most_reviewed_books_top30_df['Year-month'] = most_reviewed_books_top30_df['reviewTime'].dt.to_period('M')\n",
    "most_reviewed_books_top30_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data describing number of reviews per day for each book in top30\n",
    "most_reviewed_books_top30_df.groupby(['asin', 'reviewTime']).sum()\n",
    "                            .to_csv(\"number_of_reviews_per_day_top30_books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregating the data by month\n",
    "m_rev_books_by_month = most_reviewed_books_top30_df.groupby(['asin', 'Year-month'], as_index=True).sum()\n",
    "m_rev_books_by_month.to_csv(\"number_of_reviews_per_month_top30_books.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-line plot for top30 books - timeseries of reviews per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_rev_books_by_month.unstack(level=0).to_csv(\"number_of_reviews_per_month_top30_books_UNSTACKED.csv\")\n",
    "m_rev_books_by_month.unstack(level=0).plot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A way to use a method on each entry of a column\n",
    "fun = np.vectorize(lambda x: x.to_timestamp())\n",
    "m_rev_books_by_month['Timestamp'] = fun(m_rev_books_by_month['Year-month'].values)\n",
    "m_rev_books_by_month.set_index('asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length and number of reviews per book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aTuple = (0, 0)\n",
    "avg_len_review = df.select('asin', 'reviewText').rdd.map(lambda row: (row.asin, len(row.reviewText)))\n",
    "avg_len_review = avg_len_review.aggregateByKey(aTuple, lambda a,b: (a[0] + b, a[1] + 1), lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "avg_len_review = avg_len_review.mapValues(lambda v: (v[0]/v[1], v[1]))\n",
    "avg_len_review = avg_len_review.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation of the data to be able to load it as a DataFrame \n",
    "avg_len_review = [(k, v1, v2) for k, (v1, v2) in avg_len_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing and saving to file\n",
    "avg_len_review_per_book_df = pd.DataFrame(avg_len_review, columns=['Book_id', 'Avg_len', 'number_of_reviews'])\n",
    "avg_len_review_per_book_df.sort_values(['Avg_len', 'number_of_reviews'], ascending=False, inplace=True)\n",
    "avg_len_review_per_book_df.to_csv(\"avg_length_and_number_of_reviews_per_book.csv\", index=False)\n",
    "avg_len_review_per_book_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spark context shutdown\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
